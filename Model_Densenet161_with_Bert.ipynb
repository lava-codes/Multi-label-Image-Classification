{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import re\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils import data\n",
    "import time\n",
    "import sklearn\n",
    "import copy\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directory\n",
    "user = getpass.getuser()\n",
    "if user == 'scgst':\n",
    "    dir_home = \"C:\\\\Users\\\\scgst\\\\Documents\\\\Git\\\\COMP5329\\\\Assignment_2\\\\Code\\\\\"\n",
    "elif user == 'mgup6878':\n",
    "    dir_home = \"C:\\\\Users\\\\mgup6878\\\\Desktop\\\\Deep Learning\\\\COMP5329 Assignment 2-20200513T155933Z-001\\\\COMP5329 Assignment 2\\\\Code\\\\\"\n",
    "\n",
    "dir_input = os.path.join(dir_home, 'Input')\n",
    "dir_output = os.path.join(dir_home, 'Output')\n",
    "\n",
    "dir_data = os.path.join(dir_input, 'data')\n",
    "train_csv = os.path.join(dir_input,'train.csv')\n",
    "test_csv = os.path.join(dir_input,'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed = 27):\n",
    "    \n",
    "    \"\"\"https://pytorch.org/docs/stable/notes/randomness.html\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_all(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 50 #30\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in train and test tables\n",
    "with open(train_csv) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    train_df_full = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
    "    \n",
    "print(train_df_full.head())\n",
    "print(train_df_full.shape)\n",
    "print(\"\")\n",
    "\n",
    "with open(test_csv) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    test_df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
    "    \n",
    "print(test_df.head())\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoding(labels):\n",
    "    labels = [[int(n) for n in el ]for el in [w.split(' ') for w in labels.tolist()]]\n",
    "    \n",
    "    # Get \n",
    "    flat_list = []\n",
    "    for sublist in labels:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "            \n",
    "    unique_labels = sorted(list(set(flat_list)))\n",
    "    n_classes = len(unique_labels)\n",
    "    \n",
    "    label_dict = {l:i for i,l in enumerate(unique_labels)}\n",
    "    label_dict_revert = {i:l for i,l in enumerate(unique_labels)}\n",
    "    \n",
    "    return(n_classes, label_dict, label_dict_revert)\n",
    "\n",
    "def encode_target(labels, label_dict, n_classes):\n",
    "    labels = [[int(n) for n in el ]for el in [w.split(' ') for w in labels.tolist()]]\n",
    "    \n",
    "    labels_expanded = []\n",
    "    for el in labels:\n",
    "        label_arr = [0] * n_classes\n",
    "        for l in el:\n",
    "            d = label_dict[l]\n",
    "            label_arr[d] = 1\n",
    "        labels_expanded.append(label_arr)\n",
    "        \n",
    "    return labels_expanded\n",
    "# labels_expanded = encode_target(labels, label_dict, n_classes)\n",
    "\n",
    "def revert_encoding(labels_expanded, label_dict_revert):\n",
    "    full_map = []\n",
    "    for el in labels_expanded:\n",
    "        c = 0\n",
    "        label_revert = []\n",
    "        for l in el:\n",
    "            if (l == 1):\n",
    "                d = label_dict_revert[c]\n",
    "                d = str(d)\n",
    "                label_revert.append(d)\n",
    "            c += 1\n",
    "        s = \" \".join(label_revert)\n",
    "        full_map.append(s)\n",
    "    \n",
    "    return full_map\n",
    "\n",
    "# encode_reverted = revert_encoding(labels_expanded, label_dict_revert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_df_full['Labels']\n",
    "n_classes, label_dict, label_dict_revert = get_encoding(labels)\n",
    "print(n_classes)\n",
    "print(label_dict)\n",
    "print(label_dict_revert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "labels_expanded = encode_target(labels, label_dict, n_classes)\n",
    "\n",
    "# Add encoded labels to train table\n",
    "train_df_full['Expanded_Labels'] = labels_expanded\n",
    "train_df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and validation set\n",
    "train_df, val_df = train_test_split(train_df_full, test_size = 0.30)\n",
    "train_df = train_df.reset_index(drop = True)\n",
    "val_df = val_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_captions = train_df.iloc[:, 2]\n",
    "X_val_captions = val_df.iloc[:, 2]\n",
    "X_test_captions = test_df.iloc[:, 1]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_train_captions = [tokenizer.tokenize('[CLS] ' + sent + ' [SEP]') for sent in X_train_captions]\n",
    "X_train_tokens = [tokenizer.convert_tokens_to_ids(sent) for sent in X_train_captions]\n",
    "\n",
    "X_val_captions = [tokenizer.tokenize('[CLS] ' + sent + ' [SEP]') for sent in X_val_captions]\n",
    "X_val_tokens = [tokenizer.convert_tokens_to_ids(sent) for sent in X_val_captions]\n",
    "\n",
    "X_test_captions = [tokenizer.tokenize('[CLS] ' + sent + ' [SEP]') for sent in X_test_captions]\n",
    "X_test_tokens = [tokenizer.convert_tokens_to_ids(sent) for sent in X_test_captions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_token = -math.inf\n",
    "\n",
    "for i in range(len(X_train_tokens)):\n",
    "    if max_token < max(X_train_tokens[i]):\n",
    "        max_token = max(X_train_tokens[i])\n",
    "\n",
    "for i in range(len(X_val_tokens)):\n",
    "    if max_token < max(X_val_tokens[i]):\n",
    "        max_token = max(X_val_tokens[i])\n",
    "    \n",
    "for i in range(len(X_test_tokens)):\n",
    "    if max_token < max(X_test_tokens[i]):\n",
    "        max_token = max(X_test_tokens[i])\n",
    "        \n",
    "max_token\n",
    "\n",
    "for i in range(len(X_train_tokens)):\n",
    "    X_train_tokens[i] = [j / max_token for j in X_train_tokens[i]]\n",
    "    \n",
    "for i in range(len(X_val_tokens)):\n",
    "    X_val_tokens[i] = [j / max_token for j in X_val_tokens[i]]\n",
    "    \n",
    "for i in range(len(X_test_tokens)):\n",
    "    X_test_tokens[i] = [j / max_token for j in X_test_tokens[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max(\n",
    "    max([len(i) for i in X_train_tokens]),\n",
    "    max([len(i) for i in X_val_tokens]),\n",
    "    max([len(i) for i in X_test_tokens])\n",
    ")\n",
    "\n",
    "X_train_tokens = [i + [0] * (MAX_LEN - len(i)) for i in X_train_tokens]\n",
    "X_val_tokens = [i + [0] * (MAX_LEN - len(i)) for i in X_val_tokens]\n",
    "X_test_tokens = [i + [0] * (MAX_LEN - len(i)) for i in X_test_tokens]\n",
    "\n",
    "X_train_tokens = torch.tensor(X_train_tokens)\n",
    "X_val_tokens = torch.tensor(X_val_tokens)\n",
    "X_test_tokens = torch.tensor(X_test_tokens)\n",
    "\n",
    "print(X_train_tokens.shape)\n",
    "print(X_val_tokens.shape)\n",
    "print(X_test_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data\n",
    "class ImageData(data.Dataset):\n",
    "    def __init__(self, df, dirpath, transform, test = False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        self.dirpath = dirpath\n",
    "        self.transform = transform\n",
    "        \n",
    "        # image data \n",
    "        self.image_arr = np.asarray(str(self.dirpath) + '/' + self.df.iloc[:, 0])          \n",
    "        \n",
    "        # labels data\n",
    "        if not self.test:\n",
    "             self.label_df = self.df.iloc[:, 3]\n",
    "        \n",
    "        # Calculate length of df\n",
    "        self.data_len = len(self.df.index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_arr[idx]\n",
    "        img = Image.open(image_name)\n",
    "        img_tensor = self.transform(img)\n",
    "        if not self.test:\n",
    "            image_labels = self.label_df[idx]                \n",
    "            image_label = torch.tensor(image_labels, dtype= torch.float32)\n",
    "            return (img_tensor, image_label.squeeze())\n",
    "        \n",
    "        return (img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Image transformation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224), \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(255),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Loading data\n",
    "train_dataset = ImageData(train_df, dir_data, data_transforms['train'])\n",
    "train_loader = data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "features_train, labels_train = next(iter(train_loader))\n",
    "\n",
    "val_dataset = ImageData(val_df, dir_data, data_transforms['test'])\n",
    "val_loader = data.DataLoader(\n",
    "    dataset = val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "features_val, labels_val = next(iter(val_loader))\n",
    "\n",
    "train_full_dataset = ImageData(train_df_full, dir_data, data_transforms['train'])\n",
    "train_full_loader = data.DataLoader(\n",
    "    dataset = train_full_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "features_train_full, labels_train_full = next(iter(train_full_loader))\n",
    "\n",
    "test_dataset = ImageData(test_df, dir_data, data_transforms['test'], test = True)\n",
    "test_loader = data.DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "features_test = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Data Length: {len(train_df)}\\nMini Batch Size: {BATCH_SIZE}\\nBatch Numbers: {len(train_loader)}\\nTrain Features: {features_train.shape}\\nTrain Labels: {labels_train.shape}\")\n",
    "print()\n",
    "print(f\"Validation Data Length: {len(val_df)}\\nMini Batch Size: {BATCH_SIZE}\\nBatch Numbers: {len(val_loader)}\\nValidation Features: {features_val.shape}\\nValidation Labels: {labels_val.shape}\")\n",
    "print()\n",
    "print(f\"Full Train Data Length: {len(train_df_full)}\\nMini Batch Size: {BATCH_SIZE}\\nBatch Numbers: {len(train_full_loader)}\\nFull Train Features: {features_train_full.shape}\\nFull Train Labels: {labels_train_full.shape}\")\n",
    "print()\n",
    "print(f\"Test Data Length: {len(test_df)}\\nMini Batch Size: {BATCH_SIZE}\\nBatch Numbers: {len(test_loader)}\\nTest Features: {features_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained model using torchvision.models as models library\n",
    "model = models.densenet161(pretrained = True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')\n",
    "print()\n",
    "\n",
    "# Create new classifier for model using torch.nn as nn library\n",
    "classifier_input = model.classifier.in_features\n",
    "print('Number of Outputs from densenet161 features: ' + str(classifier_input))\n",
    "print()\n",
    "num_labels = n_classes #PUT IN THE NUMBER OF LABELS IN YOUR DATA\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(classifier_input + MAX_LEN, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 300),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(300, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, num_labels),\n",
    "    nn.LogSoftmax(dim = 1)\n",
    ")\n",
    "# Replace default classifier with new classifier\n",
    "model.classifier = classifier\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')\n",
    "\n",
    "# Move model to the device specified above\n",
    "model.to(device)\n",
    "\n",
    "# Set the error function using torch.nn as nn library\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Set the optimizer function using torch.optim as optim library\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "running_train_loss = []\n",
    "running_val_loss = []\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Training the model\n",
    "    model.train()\n",
    "    mini_batch_counter = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Print the progress of our training\n",
    "        if (mini_batch_counter % 50) == 0:\n",
    "            print(\"Epoch: {}/{} | Phase: 'Train' | Batch: {}/{} | Time: {}\".format(\n",
    "              epoch + 1,\n",
    "              NUM_EPOCHS, \n",
    "              mini_batch_counter + 1,\n",
    "              len(train_loader),\n",
    "              datetime.now()\n",
    "            ))\n",
    "        \n",
    "        # Text mini batch\n",
    "        text_train_mini_batch = X_train_tokens[mini_batch_counter * BATCH_SIZE : (mini_batch_counter + 1) * BATCH_SIZE]\n",
    "        text_train_mini_batch = text_train_mini_batch.float()     \n",
    "        # Move to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Clear optimizers\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        # output = model.forward(inputs)\n",
    "        features = model.features(inputs)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        concatenated_embeddings_torch = torch.cat((out.to(device), text_train_mini_batch.to(device)), 1)\n",
    "        output = model.classifier(concatenated_embeddings_torch)\n",
    "        # Loss\n",
    "        loss = criterion(output, labels)\n",
    "        # Calculate gradients (backpropogation)\n",
    "        loss.backward()\n",
    "        # Adjust parameters based on gradients\n",
    "        optimizer.step()\n",
    "        # Add the loss to the training set's running loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        mini_batch_counter += 1\n",
    "    \n",
    "    # Get the average loss for the entire epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)   \n",
    "    running_train_loss.append(train_loss)\n",
    "    elapsed_train_time = time.time() - start_time\n",
    "    \n",
    "    print('Epoch: {} / {} \\tTraining Loss: {:.6f} \\tTrain Time: {:.6f}mins'.format(\n",
    "        epoch + 1, NUM_EPOCHS, train_loss, elapsed_train_time / 60\n",
    "    ))\n",
    "\n",
    "    # Evaluating the model\n",
    "    model.eval()\n",
    "    mini_batch_counter = 0\n",
    "    # Tell torch not to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # Print the progress of our training\n",
    "            if (mini_batch_counter % 50) == 0:\n",
    "                print(\"Epoch: {}/{} | Phase: 'Test' | Batch: {}/{} | Time: {}\".format(\n",
    "                  epoch + 1,\n",
    "                  NUM_EPOCHS, \n",
    "                  mini_batch_counter + 1,\n",
    "                  len(val_loader),\n",
    "                  datetime.now()\n",
    "                ))\n",
    "                \n",
    "            # Text mini batch\n",
    "            text_val_mini_batch = X_val_tokens[mini_batch_counter * BATCH_SIZE : (mini_batch_counter + 1) * BATCH_SIZE]\n",
    "            text_val_mini_batch = text_val_mini_batch.float()     \n",
    "            # Move to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            # output = model.forward(inputs)\n",
    "            features = model.features(inputs)\n",
    "            out = F.relu(features, inplace = True)\n",
    "            out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "            out = torch.flatten(out, 1)\n",
    "            concatenated_embeddings_torch = torch.cat((out.to(device), text_val_mini_batch.to(device)), 1)\n",
    "            output = model.classifier(concatenated_embeddings_torch)\n",
    "            # Calculate Loss\n",
    "            valloss = criterion(output, labels)\n",
    "            # Add loss to the validation set's running loss\n",
    "            val_loss += valloss.item()*inputs.size(0)\n",
    "\n",
    "            mini_batch_counter += 1\n",
    "            \n",
    "    # Get the average loss for the entire epoch\n",
    "    valid_loss = val_loss/len(val_loader.dataset)\n",
    "    running_val_loss.append(valid_loss)\n",
    "    elapsed_test_time = time.time() - start_time - elapsed_train_time\n",
    "    \n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        best_epoch = epoch\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Print out the information\n",
    "    print('Epoch: {} / {} \\tValidation Loss: {:.6f} \\tValidation Time: {:.6f}mins'.format(\n",
    "        epoch + 1, NUM_EPOCHS, valid_loss, elapsed_test_time/60\n",
    "    ))\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(running_val_loss)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.show()\n",
    "\n",
    "print('Best Epoch is ' + str(best_epoch))\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss v.s. Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cost\n",
    "plt.plot(running_train_loss)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cost\n",
    "plt.plot(running_val_loss)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Scoring on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Get output\n",
    "start_time = time.time()\n",
    "whole_val_outputs = np.zeros((len(val_dataset), n_classes))\n",
    "whole_val_labels = np.zeros((len(val_dataset), n_classes))\n",
    "\n",
    "mini_batch_counter = 0\n",
    "for val_batch_input, val_batch_labels in val_loader:\n",
    "    if ((mini_batch_counter) % 50 == 0):\n",
    "        print(str(mini_batch_counter + 1) + '/' + str(len(val_loader)))\n",
    "\n",
    "    # Text mini batch\n",
    "    text_val_mini_batch = X_val_tokens[mini_batch_counter * BATCH_SIZE : (mini_batch_counter + 1) * BATCH_SIZE]\n",
    "    text_val_mini_batch = text_val_mini_batch.float()\n",
    "\n",
    "    # Move to device\n",
    "    val_batch_input = val_batch_input.to(device)\n",
    "    # Forward pass\n",
    "    # val_batch_output = model.forward(val_batch_input).detach().cpu().numpy()\n",
    "    features = model.features(val_batch_input)\n",
    "    out = F.relu(features, inplace = True)\n",
    "    out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "    out = torch.flatten(out, 1)\n",
    "    concatenated_embeddings_torch = torch.cat((out.to(device), text_val_mini_batch.to(device)), 1)\n",
    "    val_batch_output = model.classifier(concatenated_embeddings_torch).detach().cpu().numpy()\n",
    "    \n",
    "    val_batch_labels = val_batch_labels.detach().cpu().numpy()\n",
    "    \n",
    "    # Since our model outputs a LogSoftmax, find the real \n",
    "    # percentages by reversing the log function\n",
    "    whole_val_outputs[mini_batch_counter * BATCH_SIZE:(mini_batch_counter + 1) * BATCH_SIZE, :] = np.exp(val_batch_output)\n",
    "    whole_val_labels[mini_batch_counter * BATCH_SIZE:(mini_batch_counter + 1) * BATCH_SIZE, :] = val_batch_labels\n",
    "    mini_batch_counter += 1\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_val_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_val_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Prediction on Validation\n",
    "PERCENTILE = 99.7\n",
    "whole_val_predictions = np.zeros(whole_val_outputs.shape)\n",
    "for i in range(len(whole_val_outputs)):\n",
    "    whole_val_predictions[i, whole_val_outputs[i] > np.percentile(whole_val_outputs[i], PERCENTILE)] = 1\n",
    "\n",
    "# Calculate F1 Score on validation set\n",
    "print(sklearn.metrics.f1_score(y_true = whole_val_labels, y_pred = whole_val_predictions, average = 'weighted'))\n",
    "# # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "# print(sklearn.metrics.f1_score(y_true = whole_val_labels, y_pred = whole_val_predictions, average = 'sample'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_val_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Prediction\n",
    "# Get output\n",
    "start_time = time.time()\n",
    "whole_test_outputs = np.zeros((len(test_dataset), n_classes))\n",
    "whole_test_outputs = np.zeros((len(test_dataset), n_classes))\n",
    "mini_batch_counter = 0\n",
    "for test_batch_input in test_loader:\n",
    "    if ((mini_batch_counter) % 50 == 0):\n",
    "        print(str(mini_batch_counter + 1) + '/' + str(len(test_loader)))\n",
    "    \n",
    "    # Text mini batch\n",
    "    text_test_mini_batch = X_test_tokens[mini_batch_counter * BATCH_SIZE : (mini_batch_counter + 1) * BATCH_SIZE]\n",
    "    text_test_mini_batch = text_test_mini_batch.float()     \n",
    "\n",
    "    test_batch_input = test_batch_input.to(device)\n",
    "    \n",
    "    # Forward\n",
    "    # test_batch_output = model.forward(test_batch_input).detach().cpu().numpy()\n",
    "    features = model.features(test_batch_input)\n",
    "    out = F.relu(features, inplace = True)\n",
    "    out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "    out = torch.flatten(out, 1)\n",
    "    concatenated_embeddings_torch = torch.cat((out.to(device), text_test_mini_batch.to(device)), 1)\n",
    "\n",
    "    test_batch_output = model.classifier(concatenated_embeddings_torch).detach().cpu().numpy()\n",
    "\n",
    "    # Since our model outputs a LogSoftmax, find the real \n",
    "    # percentages by reversing the log function\n",
    "    whole_test_outputs[mini_batch_counter * BATCH_SIZE:(mini_batch_counter + 1) * BATCH_SIZE, :] = np.exp(test_batch_output)\n",
    "    mini_batch_counter += 1\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Prediction on Test\n",
    "PERCENTILE = 99.7\n",
    "whole_test_predictions = np.zeros(whole_test_outputs.shape)\n",
    "for i in range(len(whole_test_predictions)):\n",
    "    whole_test_predictions[i, whole_test_outputs[i] > np.percentile(whole_test_outputs[i], PERCENTILE)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "submission = revert_encoding(whole_test_predictions, label_dict_revert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(submission).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df['Labels'] = submission\n",
    "test_df = test_df.drop(columns = 'Caption')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(os.path.join(dir_output, 'Submission_Model_V8_densenet161_BERB.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "PATH = os.path.join(dir_output, 'Model_V8_densenet161_BERB.pth')\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
