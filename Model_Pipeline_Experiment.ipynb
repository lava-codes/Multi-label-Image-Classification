{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epHbXKLbVmP0"
   },
   "source": [
    "### Setup\n",
    "\n",
    "This involves the installation of required libraries and importing them. The colab version also includes mounting the drive and passing the paths to where the files have been stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4065,
     "status": "ok",
     "timestamp": 1590729825289,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "ovQFU_ccVmP0",
    "outputId": "3d53076b-dbf6-4ab0-f55a-70adc27bcf27",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pretrained-bert in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: regex in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from pytorch-pretrained-bert) (2020.5.14)\n",
      "Requirement already satisfied: boto3 in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from pytorch-pretrained-bert) (1.13.17)\n",
      "Requirement already satisfied: torch>=0.4.1 in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from pytorch-pretrained-bert) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from pytorch-pretrained-bert) (4.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from pytorch-pretrained-bert) (1.15.4)\n",
      "Requirement already satisfied: requests in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from pytorch-pretrained-bert) (2.21.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.17.0,>=1.16.17 in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.16.17)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from requests->pytorch-pretrained-bert) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2019.9.11)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from botocore<1.17.0,>=1.16.17->boto3->pytorch-pretrained-bert) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\scgst\\documents\\anaconda\\lib\\site-packages (from botocore<1.17.0,>=1.16.17->boto3->pytorch-pretrained-bert) (2.7.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\scgst\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.0,>=1.16.17->boto3->pytorch-pretrained-bert) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3013,
     "status": "ok",
     "timestamp": 1590729825291,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "6cup-xzGWiXx",
    "outputId": "7c6419f1-cec5-4287-d7d3-4e9866aa1ab1"
   },
   "outputs": [],
   "source": [
    "### Google-Colab Version ###\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_ZR0eYKVmP3"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import re\n",
    "from io import StringIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import sklearn\n",
    "import copy\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1590729829124,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "ORv-vOnlcTw9",
    "outputId": "bf4490fc-ffcd-4e03-bb24-f6d4452662c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Code snippet to check if any unused variable still occupies GPU Memory \n",
    "\n",
    "### RUN JUST FOR THE COLAB VERSION ###\n",
    "def pretty_size(size):\n",
    "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "\tassert(isinstance(size, torch.Size))\n",
    "\treturn \" × \".join(map(str, size)) \n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "\timport gc\n",
    "\ttotal_size = 0\n",
    "\tfor obj in gc.get_objects():\n",
    "\t\ttry:\n",
    "\t\t\tif torch.is_tensor(obj):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "\t\t\t\t\ttotal_size += obj.numel()\n",
    "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "\t\t\t\t\ttotal_size += obj.data.numel()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass        \n",
    "\tprint(\"Total size:\", total_size)\n",
    "\n",
    "dump_tensors()\n",
    "torch.cuda.empty_cache()\n",
    "import gc \n",
    "model = None\n",
    "learn = None\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VCmmsq7IVmP5"
   },
   "outputs": [],
   "source": [
    "### RUN FOR THE JUPYTER VERSION ###\n",
    "\n",
    "## Directories\n",
    "user = getpass.getuser()\n",
    "if user == 'scgst':\n",
    "    dir_home = \"C:\\\\Users\\\\scgst\\\\Documents\\\\Git\\\\COMP5329\\\\Assignment_2\\\\Code\\\\\"\n",
    "elif user == 'mgup6878':\n",
    "    dir_home = \"C:\\\\Users\\\\mgup6878\\\\Desktop\\\\Deep Learning\\\\COMP5329 Assignment 2-20200513T155933Z-001\\\\COMP5329 Assignment 2\\\\Code\\\\\"\n",
    "elif user == 'root':\n",
    "    dir_home = '/content/drive/My Drive/COMP5329 Assignment 2-20200513T155933Z-001.zip (Unzipped Files)/COMP5329 Assignment 2/Code/'\n",
    "\n",
    "dir_input = os.path.join(dir_home, 'Input')\n",
    "dir_output = os.path.join(dir_home, 'Output')\n",
    "\n",
    "dir_data = os.path.join(dir_input, 'data')\n",
    "if user == 'root':\n",
    "    dir_data = os.path.join(dir_input, 'Data2')\n",
    "\n",
    "train_csv = os.path.join(dir_input,'train.csv')\n",
    "test_csv = os.path.join(dir_input,'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3llUtvZHVmP8"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed = 27):\n",
    "    \n",
    "    \"\"\"https://pytorch.org/docs/stable/notes/randomness.html\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_all(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yqogHMmVZkcI"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qN2mqpePVmP-"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 50 # 200 if google colab #30\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "USE_BERT = False\n",
    "USE_OVER_SAMPLING = False\n",
    "\n",
    "TRAIN_TEXT = False\n",
    "\n",
    "# GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhRIt_hDVmQA"
   },
   "source": [
    "### Loading Data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1590729846792,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "VViSjCREVmQA",
    "outputId": "06165e13-0ae6-4762-9bef-918dbf35ac72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ImageID  Labels                                            Caption\n",
      "0   0.jpg       1   Woman in swim suit holding parasol on sunny day.\n",
      "1   1.jpg    1 19  A couple of men riding horses on top of a gree...\n",
      "2   2.jpg       1  They are brave for riding in the jungle on tho...\n",
      "3   3.jpg  8 3 13  a black and silver clock tower at an intersect...\n",
      "4   4.jpg   8 3 7   A train coming to a stop on the tracks out side.\n",
      "(29996, 3)\n",
      "\n",
      "     ImageID                                            Caption\n",
      "0  30000.jpg  A little girl waring a krispy kreme hat holdin...\n",
      "1  30001.jpg  A beautiful young woman holding an orange fris...\n",
      "2  30002.jpg  A group of people sitting on couch next to a c...\n",
      "3  30003.jpg         A person on a snowboard rides on the hill.\n",
      "4  30004.jpg  A man riding a skateboard with a helmet on in ...\n",
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "## Read in train and test tables\n",
    "with open(train_csv) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    train_df_full = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
    "    \n",
    "print(train_df_full.head())\n",
    "print(train_df_full.shape)\n",
    "print(\"\")\n",
    "\n",
    "with open(test_csv) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    test_df = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
    "    \n",
    "print(test_df.head())\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PT6Zdsv3VmQD"
   },
   "source": [
    "### Encoding\n",
    "The no. of labels are 18  - [1,2,3,4,5,6,7,8,9,10,11,13,14,15,16,17,18,19]\n",
    "The labels present in given train_data is in the form of space separated strings. These are split into lists and then later one-hot encoded to convert them to format acceptable by the model. The functions below generate *get_encoding(), encode_target(), revert_encoding()* . Their functions are explained in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sf3jVR39VmQD"
   },
   "outputs": [],
   "source": [
    "def get_encoding(labels):\n",
    "    labels = [[int(n) for n in el ]for el in [w.split(' ') for w in labels.tolist()]]\n",
    "    '''\n",
    "    This function aims to get 2 dictionaries to encode and decode the labels. \n",
    "    Input: Labels column of data\n",
    "    Returns the no. of classes, the encoding dictionary 'label_dict' and decoding dictionary 'label_dict_revert'\n",
    "    '''\n",
    "    # Get \n",
    "    flat_list = []\n",
    "    for sublist in labels:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "            \n",
    "    unique_labels = sorted(list(set(flat_list)))\n",
    "    n_classes = len(unique_labels)\n",
    "    \n",
    "    label_dict = {l:i for i,l in enumerate(unique_labels)}\n",
    "    label_dict_revert = {i:l for i,l in enumerate(unique_labels)}\n",
    "    \n",
    "    return(n_classes, label_dict, label_dict_revert)\n",
    "\n",
    "def encode_target(labels, label_dict, n_classes):\n",
    "    '''\n",
    "    This function aims to encode the labels column before training the models\n",
    "    Input: Labels column of data, encoding dictionary, no. of classes\n",
    "    Returns one-hot encoded labels column 'labels_expanded'\n",
    "    '''\n",
    "\n",
    "    labels = [[int(n) for n in el ]for el in [w.split(' ') for w in labels.tolist()]]\n",
    "    \n",
    "    labels_expanded = []\n",
    "    for el in labels:\n",
    "        label_arr = [0] * n_classes\n",
    "        for l in el:\n",
    "            d = label_dict[l]\n",
    "            label_arr[d] = 1\n",
    "        labels_expanded.append(label_arr)\n",
    "        \n",
    "    return labels_expanded\n",
    "# labels_expanded = encode_target(labels, label_dict, n_classes)\n",
    "\n",
    "def revert_encoding(labels_expanded, label_dict_revert):\n",
    "    '''\n",
    "    This function aims to decode the labels column before the evaluation of the model\n",
    "    Input: Encoded labels column of data, decoding dictionary\n",
    "    Returns labels in the original format\n",
    "    '''\n",
    "    full_map = []\n",
    "    for el in labels_expanded:\n",
    "        c = 0\n",
    "        label_revert = []\n",
    "        for l in el:\n",
    "            if (l == 1):\n",
    "                d = label_dict_revert[c]\n",
    "                d = str(d)\n",
    "                label_revert.append(d)\n",
    "            c += 1\n",
    "        s = \" \".join(label_revert)\n",
    "        full_map.append(s)\n",
    "    \n",
    "    return full_map\n",
    "\n",
    "# encode_reverted = revert_encoding(labels_expanded, label_dict_revert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1590729849417,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "nI7heYSVVmQF",
    "outputId": "271d6e28-725c-4a6d-ef16-0747a65fd8fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "{1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 13: 11, 14: 12, 15: 13, 16: 14, 17: 15, 18: 16, 19: 17}\n",
      "{0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10, 10: 11, 11: 13, 12: 14, 13: 15, 14: 16, 15: 17, 16: 18, 17: 19}\n"
     ]
    }
   ],
   "source": [
    "labels = train_df_full['Labels']\n",
    "n_classes, label_dict, label_dict_revert = get_encoding(labels)\n",
    "print(n_classes)\n",
    "print(label_dict)\n",
    "print(label_dict_revert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 751,
     "status": "ok",
     "timestamp": 1590729850523,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "oH6DGeqyVmQG",
    "outputId": "b366deb3-8394-4ca4-c13c-6103e8d22262",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageID</th>\n",
       "      <th>Labels</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Expanded_Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Woman in swim suit holding parasol on sunny day.</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>1 19</td>\n",
       "      <td>A couple of men riding horses on top of a gree...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>They are brave for riding in the jungle on tho...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>8 3 13</td>\n",
       "      <td>a black and silver clock tower at an intersect...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>8 3 7</td>\n",
       "      <td>A train coming to a stop on the tracks out side.</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ImageID  Labels                                            Caption  \\\n",
       "0   0.jpg       1   Woman in swim suit holding parasol on sunny day.   \n",
       "1   1.jpg    1 19  A couple of men riding horses on top of a gree...   \n",
       "2   2.jpg       1  They are brave for riding in the jungle on tho...   \n",
       "3   3.jpg  8 3 13  a black and silver clock tower at an intersect...   \n",
       "4   4.jpg   8 3 7   A train coming to a stop on the tracks out side.   \n",
       "\n",
       "                                     Expanded_Labels  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "4  [0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode labels\n",
    "labels_expanded = encode_target(labels, label_dict, n_classes)\n",
    "\n",
    "# Add encoded labels to train table\n",
    "train_df_full['Expanded_Labels'] = labels_expanded\n",
    "train_df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B_o2qnT7VmQI"
   },
   "source": [
    "### BERT Embeddings\n",
    "The following code aims to get the sentence embeddings for each image caption for both train and test datasets. For this we simply use the pre-trained BERT model 'bert-base-uncased'. The model is then ran in the *eval()* mode to get the embeddings. The *get_bert_embeddings()* contains the implementation to preprocess the data, get the word embeddings for each caption from the second to the last hidden layers and averaging them to get one sentence embedding. As the dataset is quite large, to avoid memory issues, we are splitting the dataset to get embeddings batchwise, then combining them to make a single tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dSVxmR1QlXzQ"
   },
   "source": [
    "#### Getting the BERT embeddings from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18253,
     "status": "ok",
     "timestamp": 1590689353599,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "dn0Q2GT_VmQJ",
    "outputId": "ba18602b-8193-4a02-dec9-bc11c0b22715"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "text_model = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RDQAR27gVmQL"
   },
   "outputs": [],
   "source": [
    "def get_bert_embeddings(X_captions, MAX_LEN, tokenizer, model):\n",
    "    tokenized_list = []\n",
    "    ids_list = []\n",
    "    seg_id_list = []\n",
    "    for sent in X_captions:\n",
    "\n",
    "        tokenize = tokenizer.tokenize('[CLS] ' + sent + ' [SEP]')\n",
    "        \n",
    "        if len(tokenize) > MAX_LEN : tokenize  = tokenize[:MAX_LEN]\n",
    "            \n",
    "        ids = tokenizer.convert_tokens_to_ids(tokenize)\n",
    "            \n",
    "        ids = torch.tensor(ids + [0] * (MAX_LEN - len(ids)))\n",
    "\n",
    "        segments_ids = torch.tensor([1]* MAX_LEN)\n",
    "        \n",
    "        seg_id_list.append(segments_ids)\n",
    "        tokenized_list.append(tokenize)\n",
    "        ids_list.append(ids)\n",
    "\n",
    "    \n",
    "    tokens_tensor = torch.stack(ids_list)\n",
    "    segments_tensors = torch.stack(seg_id_list)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "    model.eval()\n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor.to(device), segments_tensors.to(device))\n",
    "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "    \n",
    "    \n",
    "    token_vecs = encoded_layers[11]\n",
    "    # Calculate the average of all 59 token vectors.\n",
    "    sentence_embeddings = torch.mean(token_vecs, dim=1)\n",
    "    \n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lm8Ya16kVmQM"
   },
   "outputs": [],
   "source": [
    "## Code to concat list of tensors to one tensor\n",
    "#### Code cell 3\n",
    "def all_dataset_embeddings(X_captions, BATCH_SIZE, MAX_LEN, tokenizer, model,mode = 'training'):\n",
    "    n_batches = math.ceil(len(X_captions)/BATCH_SIZE)\n",
    "    #sent_emb_list = [get_bert_embeddings(X_captions[i * BATCH_SIZE:(i+1)*BATCH_SIZE], MAX_LEN, tokenizer,model) for i in range(0, n_batches)]\n",
    "    sent_emb_list = []\n",
    "    for i in range(0, n_batches):\n",
    "        sent_emb = get_bert_embeddings(X_captions[i * BATCH_SIZE:(i+1)*BATCH_SIZE], MAX_LEN, tokenizer,model)     \n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"Text Batch Process for {} set: {}/{} | Time: {}\".format(\n",
    "                mode,\n",
    "                str(i),\n",
    "                str(n_batches),\n",
    "                datetime.now()\n",
    "            ))\n",
    "        sent_emb_list.append(sent_emb)\n",
    "    return torch.cat(sent_emb_list, dim = 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the sentence embeddings\n",
    "After saving the embeddings once, we no longer would want to run the code again to get same results. To avoid the hassle, we will load the saved embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 256198,
     "status": "ok",
     "timestamp": 1590690084726,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "jwGsq7y2VmQO",
    "outputId": "8ed90fe6-2751-4d17-d723-5932ad959801",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "TEXT_BATCH_SIZE = 1000\n",
    "MAX_LEN = 59\n",
    "# Load pre-trained BERT model\n",
    "text_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "if USE_BERT:\n",
    "    if TRAIN_TEXT:\n",
    "        train_emb_list= all_dataset_embeddings(\n",
    "            train_df_full.iloc[:, 2], TEXT_BATCH_SIZE , MAX_LEN, tokenizer, text_model, 'training'\n",
    "        )\n",
    "        test_emb_list =  all_dataset_embeddings(\n",
    "            test_df.iloc[:, 1], TEXT_BATCH_SIZE, MAX_LEN, tokenizer, text_model, 'testing'\n",
    "        )\n",
    "        \n",
    "        ## Saving the embeddings for later use\n",
    "        torch.save(train_emb_list, os.path.join(dir_output, 'train_emb.pt'))\n",
    "        torch.save(test_emb_list, os.path.join(dir_output, 'test_emb.pt'))\n",
    "        \n",
    "    else:\n",
    "        train_emb_list = torch.load(os.path.join(dir_output, 'train_emb.pt'))\n",
    "        test_emb_list = torch.load(os.path.join(dir_output, 'test_emb.pt'))\n",
    "       \n",
    "    TEXT_LENGTH = train_emb_list.shape[1]\n",
    "    print(train_emb_list.shape, test_emb_list.shape)\n",
    "    \n",
    "else:\n",
    "    TEXT_LENGTH = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3vBT2vmIVmQU"
   },
   "source": [
    "### Class Exploratory\n",
    "\n",
    "Exploratory Data Analysis results reveal that the dataset is highly imbalanced due to bias towards label '1'. Based on manual analysis, the dataset seems to be a computer-vision challenge to find identify multiple entities in an image. For example -- humans, cats, trains, bikes etc. Some images have multi-labels which some have just have pure labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 933,
     "status": "ok",
     "timestamp": 1590729867600,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "QEW4IfFtVmQU",
    "outputId": "252fb712-520f-4231-d056-c7dda5a8e29a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22794  1162  4364  1272  1130  1394  1221  2210  1042  1471   604   605\n",
      "   251  1934  1099  1430  1525  1020]\n"
     ]
    }
   ],
   "source": [
    "# Class distribution before sample\n",
    "print(np.sum(labels_expanded, axis = 0))\n",
    "\n",
    "# Classes are unbalanced\n",
    "# Label one has more chances to be classfield \n",
    "# Up sampling cases with data augmentation to potentially resolve the unbalance issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1590729869020,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "ymW6GPl6fAGd",
    "outputId": "e30ee11c-f4cb-4580-a028-ce054740d833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22794\n",
      "[1.0, 19.61617900172117, 5.223189734188818, 17.919811320754718, 20.171681415929203, 16.351506456241033, 18.668304668304668, 10.314027149321268, 21.87523992322457, 15.49558123725357, 37.73841059602649, 37.67603305785124, 90.81274900398407, 11.785935884177869, 20.740673339399454, 15.93986013986014, 14.946885245901639, 22.347058823529412]\n"
     ]
    }
   ],
   "source": [
    "## Assigning weights to each class such that dominant class is weighed less and less dominant are weighted more\n",
    "## These weights can be used to manage the data imbalance.\n",
    "## This technique wasn't found to be much useful for our dataset. Hence Data Augmentation was used for our dataset.\n",
    "tot_list = list(np.sum(labels_expanded, axis = 0))\n",
    "max_sample = max(tot_list)\n",
    "weights_per_label = [max_sample/n for n in tot_list]\n",
    "print(max_sample)\n",
    "print(weights_per_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IArhnwcVmQW"
   },
   "source": [
    "### Over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0OKr3rQkVmQW"
   },
   "outputs": [],
   "source": [
    "if USE_OVER_SAMPLING:\n",
    "    # Index for cases have label one and do not have label one\n",
    "    NO_LABEL_ONE_INDEX = []\n",
    "    LABEL_ONE_INDEX = []\n",
    "    for i in range(len(labels_expanded)):\n",
    "        el = labels_expanded[i]\n",
    "        if el[0] == 0:\n",
    "            NO_LABEL_ONE_INDEX.append(i)\n",
    "        else:\n",
    "            LABEL_ONE_INDEX.append(i)\n",
    "\n",
    "    # Sampling cases not having label one\n",
    "    TIMES_TO_EXTRACT_UNBALANCE_CALSS = 10\n",
    "    SAMPLE_SIZE = 5000\n",
    "    SAMPLE_NON_LABEL_ONE_INDEX = []\n",
    "    for i in range(TIMES_TO_EXTRACT_UNBALANCE_CALSS):\n",
    "        SAMPLE_NON_LABEL_ONE_INDEX.append(np.random.choice(NO_LABEL_ONE_INDEX, SAMPLE_SIZE))\n",
    "    SAMPLE_NON_LABEL_ONE_INDEX = np.concatenate(SAMPLE_NON_LABEL_ONE_INDEX)\n",
    "    FULL_DATA_INDEX = np.concatenate([LABEL_ONE_INDEX, NO_LABEL_ONE_INDEX, SAMPLE_NON_LABEL_ONE_INDEX])\n",
    "    # Class distribution before sample\n",
    "    CLASS_AFTER_SAMPLE = []\n",
    "    for i in FULL_DATA_INDEX:\n",
    "        CLASS_AFTER_SAMPLE.append(labels_expanded[i])\n",
    "    print(np.sum(CLASS_AFTER_SAMPLE, axis = 0))\n",
    "    train_df_full_new = pd.DataFrame(\n",
    "        train_df_full, \n",
    "        columns = ['ImageID' , 'Labels', 'Caption', 'Expanded_Labels'],\n",
    "        index = FULL_DATA_INDEX\n",
    "    ) \n",
    "    \n",
    "    train_df_full = train_df_full_new.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3T19_yMVmQY"
   },
   "source": [
    "### Data Partition\n",
    "\n",
    "The  data was split into train and validation sets in the ratio 70:30. Hence, the no. of examples in train set were 20,997 and that in validation set 8,999. The trained BERT embeddings were split in the same ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tl9L9_6YVmQY"
   },
   "outputs": [],
   "source": [
    "ALL_INDEX = range(0, len(train_df_full))\n",
    "TRAIN_INDEX, VAL_INDEX = train_test_split(ALL_INDEX, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCQfK6_TVmQa"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(\n",
    "    train_df_full, \n",
    "    columns = ['ImageID' , 'Labels', 'Caption', 'Expanded_Labels'],\n",
    "    index = TRAIN_INDEX\n",
    ") \n",
    "train_df = train_df.reset_index(drop = True)\n",
    "\n",
    "val_df = pd.DataFrame(\n",
    "    train_df_full, \n",
    "    columns = ['ImageID' , 'Labels', 'Caption', 'Expanded_Labels'],\n",
    "    index = VAL_INDEX\n",
    ") \n",
    "val_df = val_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJqra4L8VmQb"
   },
   "outputs": [],
   "source": [
    "if USE_BERT:\n",
    "    train_emb = train_emb_list[TRAIN_INDEX]\n",
    "    val_emb = train_emb_list[VAL_INDEX]\n",
    "    test_emb = test_emb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8ipxUZKVmQd"
   },
   "source": [
    "### Data Extraction\n",
    "\n",
    "The following *ImageData* class aims to build a custom dataset in a format that can be used by the dataloader. It takes the dataframe as an input which can be the train/validation/test sets containing the ImageId column containing image file names and the one-hot-encoded labels(only for the train/validation sets), the path to the folder containing the images, and a parameter 'test' which if true indicates it is a test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3PH2xyRwVmQd"
   },
   "outputs": [],
   "source": [
    "# Extract Data\n",
    "class ImageData(data.Dataset):\n",
    "    def __init__(self, df, dirpath, transform, test = False):\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "        self.dirpath = dirpath\n",
    "        self.transform = transform\n",
    "        \n",
    "        # image data \n",
    "        self.image_arr = np.asarray(str(self.dirpath) + '/' + self.df.iloc[:, 0])          \n",
    "        \n",
    "        # labels data\n",
    "        if not self.test:\n",
    "             self.label_df = self.df.iloc[:, 3]\n",
    "        \n",
    "        # Calculate length of df\n",
    "        self.data_len = len(self.df.index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_arr[idx]\n",
    "        img = Image.open(image_name)\n",
    "        img_tensor = self.transform(img)\n",
    "        if not self.test:\n",
    "            image_labels = self.label_df[idx]                \n",
    "            image_label = torch.tensor(image_labels, dtype= torch.float32)\n",
    "            return (img_tensor, image_label.squeeze())\n",
    "        \n",
    "        return (img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKj-K41qVmQf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Image transformation \n",
    "### The following code mentions the data augmentation transformations applied to images in each of the datasets.\n",
    "### Each dataset is the loaded onto respective dataloaders to be used while training\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(255),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p = 0.5),\n",
    "#         transforms.RandomVerticalFlip(p = 0.5),\n",
    "#         transforms.RandomRotation(degrees = [-45, 45]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(255),\n",
    "        transforms.RandomResizedCrop(224), \n",
    "        transforms.RandomHorizontalFlip(p = 0.5),\n",
    "#         transforms.RandomVerticalFlip(p = 0.5),\n",
    "#         transforms.RandomRotation(degrees = [-45, 45]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(255),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Loading data\n",
    "train_dataset = ImageData(train_df, dir_data, data_transforms['train'])\n",
    "train_loader = data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "features_train, labels_train = next(iter(train_loader))\n",
    "\n",
    "val_dataset = ImageData(val_df, dir_data, data_transforms['val'])\n",
    "val_loader = data.DataLoader(\n",
    "    dataset = val_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "features_val, labels_val = next(iter(val_loader))\n",
    "\n",
    "train_full_dataset = ImageData(train_df_full, dir_data, data_transforms['train'])\n",
    "train_full_loader = data.DataLoader(\n",
    "    dataset = train_full_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "features_train_full, labels_train_full = next(iter(train_full_loader))\n",
    "\n",
    "test_dataset = ImageData(test_df, dir_data, data_transforms['test'], test = True)\n",
    "test_loader = data.DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = False\n",
    ")\n",
    "features_test = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1590728121031,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "5VgC3L2SVmQh",
    "outputId": "444076ca-08d3-4b16-9002-7efa711a79d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Length: 20997\n",
      "Mini Batch Size: 50\n",
      "Batch Numbers: 420\n",
      "Train Features: torch.Size([50, 3, 224, 224])\n",
      "Train Labels: torch.Size([50, 18])\n",
      "\n",
      "Validation Data Length: 8999\n",
      "Mini Batch Size: 50\n",
      "Batch Numbers: 180\n",
      "Validation Features: torch.Size([50, 3, 224, 224])\n",
      "Validation Labels: torch.Size([50, 18])\n",
      "\n",
      "Full Train Data Length: 29996\n",
      "Mini Batch Size: 50\n",
      "Batch Numbers: 600\n",
      "Full Train Features: torch.Size([50, 3, 224, 224])\n",
      "Full Train Labels: torch.Size([50, 18])\n",
      "\n",
      "Test Data Length: 10000\n",
      "Mini Batch Size: 50\n",
      "Batch Numbers: 200\n",
      "Test Features: torch.Size([50, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Data Length: {len(train_df)}\\nMini Batch Size: {BATCH_SIZE}\\nBatch Numbers: {len(train_loader)}\\nTrain Features: {features_train.shape}\\nTrain Labels: {labels_train.shape}\")\n",
    "print()\n",
    "print(f\"Validation Data Length: {len(val_df)}\\nMini Batch Size: {BATCH_SIZE}\\nBatch Numbers: {len(val_loader)}\\nValidation Features: {features_val.shape}\\nValidation Labels: {labels_val.shape}\")\n",
    "print()\n",
    "print(f\"Full Train Data Length: {len(train_df_full)}\\nMini Batch Size: {BATCH_SIZE}\\nBatch Numbers: {len(train_full_loader)}\\nFull Train Features: {features_train_full.shape}\\nFull Train Labels: {labels_train_full.shape}\")\n",
    "print()\n",
    "print(f\"Test Data Length: {len(test_df)}\\nMini Batch Size: {BATCH_SIZE}\\nBatch Numbers: {len(test_loader)}\\nTest Features: {features_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSTrPrZnVmQj"
   },
   "source": [
    "### Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HqVsneDyVmQk"
   },
   "outputs": [],
   "source": [
    "## Uncomment to empty the GPU cache ; Alternatively, you may restart the session and clear all the outputs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "638d7a4fb0d44c69ab6b1aad232197eb",
      "121b90dfecb144408848936c69037117",
      "0a742eef6fb94bd6a3b2168c8dd0cd26",
      "b075521abc2242319b602329d96ed8cb",
      "3137518932594eb4b3116d2f78e512eb",
      "4dbfc33eb9054f8da948d6484d789d9c",
      "12ad520349564ad3bb9e680832cc1ca8",
      "cfbb4d6847b44a8080e0e862f5cde5c4"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4125,
     "status": "ok",
     "timestamp": 1590729381551,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "Fv6qgTz8VmQm",
    "outputId": "82a9b17a-618a-4a65-ee79-eadc38db1d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28,681,000 total parameters.\n",
      "0 training parameters.\n",
      "\n",
      "Number of Outputs from densenet161 features: 2208\n",
      "\n",
      "29,494,834 total parameters.\n",
      "3,022,834 training parameters.\n"
     ]
    }
   ],
   "source": [
    "# Get pretrained model using torchvision.models as models library\n",
    "## We use the densenet161 pre-trained model\n",
    "\n",
    "model = models.densenet161(pretrained = True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')\n",
    "print()\n",
    "\n",
    "# Create new classifier for model using torch.nn as nn library\n",
    "classifier_input = model.classifier.in_features\n",
    "print('Number of Outputs from densenet161 features: ' + str(classifier_input))\n",
    "print()\n",
    "\n",
    "#PUT IN THE NUMBER OF LABELS IN THE DATA ; in our case 18\n",
    "num_labels = n_classes  \n",
    "\n",
    "#The input to the classifier model will be the concatenated image tensor and the corresponding BERT embedding. \n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(classifier_input + TEXT_LENGTH, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 300),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(300, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, num_labels),\n",
    "    nn.Sigmoid() # LogSoftmax(dim = 1)\n",
    ")\n",
    "# Replace default classifier with new classifier\n",
    "model.classifier = classifier\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')\n",
    "\n",
    "# Move model to the device specified above\n",
    "model.to(device)\n",
    "\n",
    "# Set the error function using torch.nn as nn library\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Set the optimizer function using torch.optim as optim library\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr = LEARNING_RATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "naJndJga__0g"
   },
   "outputs": [],
   "source": [
    "weights_per_label = torch.FloatTensor(weights_per_label).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwprxEuCVmQn"
   },
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31209,
     "status": "error",
     "timestamp": 1590729545806,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "6pkvdp9wVmQo",
    "outputId": "dc4a689e-e99e-4289-9105-5ed00b5b9e8f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "running_train_loss = []\n",
    "running_val_loss = []\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Training the model\n",
    "    model.train()\n",
    "    mini_batch_counter = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Print the progress of our training\n",
    "        if (mini_batch_counter % 50) == 0:\n",
    "            print(\"Epoch: {}/{} | Phase: 'Train' | Batch: {}/{} | Time: {}\".format(\n",
    "              epoch + 1,\n",
    "              NUM_EPOCHS, \n",
    "              mini_batch_counter + 1,\n",
    "              len(train_loader),\n",
    "              datetime.now()\n",
    "            ))\n",
    "        \n",
    "        # Text mini batch\n",
    "        text_train_mini_batch = train_emb[mini_batch_counter * BATCH_SIZE : (mini_batch_counter + 1) * BATCH_SIZE] \n",
    "        # Move to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # Clear optimizers\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        features = model.features(inputs)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        concatenated_embeddings_torch = torch.cat((out.to(device), text_train_mini_batch.to(device)), 1)\n",
    "        output = model.classifier(concatenated_embeddings_torch)\n",
    "        # Loss\n",
    "        loss = criterion(torch.sigmoid(output), labels)\n",
    "        # Calculate gradients (backpropogation)\n",
    "        loss.backward()\n",
    "        # Adjust parameters based on gradients\n",
    "        optimizer.step()\n",
    "        # Add the loss to the training set's running loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        mini_batch_counter += 1\n",
    "    \n",
    "    # Get the average loss for the entire epoch\n",
    "    train_loss = train_loss / len(train_loader.dataset)   \n",
    "    running_train_loss.append(train_loss)\n",
    "    elapsed_train_time = time.time() - start_time\n",
    "    \n",
    "    print('Epoch: {} / {} \\tTraining Loss: {:.6f} \\tTrain Time: {:.6f}mins'.format(\n",
    "        epoch + 1, NUM_EPOCHS, train_loss, elapsed_train_time / 60\n",
    "    ))\n",
    "\n",
    "    # Evaluating the model\n",
    "    model.eval()\n",
    "    mini_batch_counter = 0\n",
    "    # Tell torch not to calculate gradients\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # Print the progress of our training\n",
    "            if (mini_batch_counter % 50) == 0:\n",
    "                print(\"Epoch: {}/{} | Phase: 'Test' | Batch: {}/{} | Time: {}\".format(\n",
    "                  epoch + 1,\n",
    "                  NUM_EPOCHS, \n",
    "                  mini_batch_counter + 1,\n",
    "                  len(val_loader),\n",
    "                  datetime.now()\n",
    "                ))\n",
    "                \n",
    "            # Text mini batch\n",
    "            text_val_mini_batch = val_emb[mini_batch_counter * BATCH_SIZE : (mini_batch_counter + 1) * BATCH_SIZE]   \n",
    "            # Move to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            features = model.features(inputs)\n",
    "            out = F.relu(features, inplace = True)\n",
    "            out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "            out = torch.flatten(out, 1)\n",
    "            concatenated_embeddings_torch = torch.cat((out.to(device), text_val_mini_batch.to(device)), 1)\n",
    "            output = model.classifier(concatenated_embeddings_torch)\n",
    "            # Calculate Loss\n",
    "            valloss = criterion(torch.sigmoid(output), labels)\n",
    "            # Add loss to the validation set's running loss\n",
    "            val_loss += valloss.item()*inputs.size(0)\n",
    "\n",
    "            mini_batch_counter += 1\n",
    "            \n",
    "    # Get the average loss for the entire epoch\n",
    "    valid_loss = val_loss/len(val_loader.dataset)\n",
    "    running_val_loss.append(valid_loss)\n",
    "    elapsed_test_time = time.time() - start_time - elapsed_train_time\n",
    "    \n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        best_epoch = epoch\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Print out the information\n",
    "    print('Epoch: {} / {} \\tValidation Loss: {:.6f} \\tValidation Time: {:.6f}mins'.format(\n",
    "        epoch + 1, NUM_EPOCHS, valid_loss, elapsed_test_time/60\n",
    "    ))\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(running_val_loss)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.show()\n",
    "\n",
    "print('Best Epoch is ' + str(best_epoch))\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jN0KyQsuVmQp"
   },
   "source": [
    "### Loss v.s. Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1590713337398,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "uptK5D5WVmQq",
    "outputId": "30c5d38c-1f41-4ab7-a950-6a4e260716ec"
   },
   "outputs": [],
   "source": [
    "# plot the cost\n",
    "plt.plot(running_train_loss)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1590713340991,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "csiUfAReVmQr",
    "outputId": "b5735bc6-7d4e-4f4f-92ad-db897b3f8183"
   },
   "outputs": [],
   "source": [
    "# plot the cost\n",
    "plt.plot(running_val_loss)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJegieToVmQu"
   },
   "source": [
    "### Prediction and Scoring on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 266863,
     "status": "ok",
     "timestamp": 1590713611084,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "MGhdwtXyVmQu",
    "outputId": "5e8cd6c0-8b3c-491b-c761-8c9f73f234a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Get output\n",
    "start_time = time.time()\n",
    "whole_val_outputs = np.zeros((len(val_dataset), n_classes))\n",
    "whole_val_labels = np.zeros((len(val_dataset), n_classes))\n",
    "\n",
    "mini_batch_counter = 0\n",
    "for val_batch_input, val_batch_labels in val_loader:\n",
    "    if ((mini_batch_counter) % 50 == 0):\n",
    "        print(str(mini_batch_counter + 1) + '/' + str(len(val_loader)))\n",
    "\n",
    "    # Text mini batch\n",
    "    text_val_mini_batch = val_emb[mini_batch_counter * BATCH_SIZE : (mini_batch_counter + 1) * BATCH_SIZE]\n",
    "    # Move to device\n",
    "    val_batch_input = val_batch_input.to(device)\n",
    "    # Forward pass\n",
    "    features = model.features(val_batch_input)\n",
    "    out = F.relu(features, inplace = True)\n",
    "    out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "    out = torch.flatten(out, 1)\n",
    "    concatenated_embeddings_torch = torch.cat((out.to(device), text_val_mini_batch.to(device)), 1)\n",
    "    \n",
    "    val_batch_output = model.classifier(concatenated_embeddings_torch).detach().cpu().numpy()\n",
    "    val_batch_labels = val_batch_labels.detach().cpu().numpy()\n",
    "    \n",
    "    # Since our model outputs a LogSoftmax, find the real \n",
    "    # percentages by reversing the log function\n",
    "    whole_val_outputs[mini_batch_counter * BATCH_SIZE:(mini_batch_counter + 1) * BATCH_SIZE, :] = np.exp(val_batch_output)\n",
    "    whole_val_labels[mini_batch_counter * BATCH_SIZE:(mini_batch_counter + 1) * BATCH_SIZE, :] = val_batch_labels\n",
    "    mini_batch_counter += 1\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2044,
     "status": "ok",
     "timestamp": 1590713949368,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "bVyJEFi-VmQz",
    "outputId": "bf683a1e-1cc8-44f8-f259-bef2919ac0b8"
   },
   "outputs": [],
   "source": [
    "# Get Prediction on Validation\n",
    "\n",
    "# # Calculate F1 Score on validation set\n",
    "# whole_val_predictions = np.round(whole_val_outputs)\n",
    "# print(sklearn.metrics.f1_score(\n",
    "#    y_true = whole_val_labels, y_pred = whole_val_predictions, average = 'weighted'\n",
    "# ))\n",
    "\n",
    "PERCENTILE = 99.7\n",
    "whole_val_predictions = np.zeros(whole_val_outputs.shape)\n",
    "for i in range(len(whole_val_outputs)):\n",
    "     whole_val_predictions[i, whole_val_outputs[i] > np.percentile(whole_val_outputs[i], PERCENTILE)] = 1\n",
    "# Calculate F1 Score on validation set\n",
    "print(sklearn.metrics.f1_score(\n",
    "    y_true = whole_val_labels, y_pred = whole_val_predictions, average = 'weighted'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwFn6oZ3VmQ0"
   },
   "outputs": [],
   "source": [
    "whole_val_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQ5awb19VmQ2"
   },
   "source": [
    "### Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wi4pDBfiVmQ2"
   },
   "outputs": [],
   "source": [
    "# Final Prediction\n",
    "# Get output\n",
    "start_time = time.time()\n",
    "whole_test_outputs = np.zeros((len(test_dataset), n_classes))\n",
    "whole_test_outputs = np.zeros((len(test_dataset), n_classes))\n",
    "mini_batch_counter = 0\n",
    "for test_batch_input in test_loader:\n",
    "    if ((mini_batch_counter) % 50 == 0):\n",
    "        print(str(mini_batch_counter + 1) + '/' + str(len(test_loader)))\n",
    "    \n",
    "    # Text mini batch\n",
    "    text_test_mini_batch = test_emb[mini_batch_counter * BATCH_SIZE : (mini_batch_counter + 1) * BATCH_SIZE]\n",
    "    # Forward\n",
    "    features = model.features(test_batch_input.to(device))\n",
    "    out = F.relu(features, inplace = True)\n",
    "    out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "    out = torch.flatten(out, 1)\n",
    "    concatenated_embeddings_torch = torch.cat((out.to(device), text_test_mini_batch.to(device)), 1)\n",
    "    test_batch_output = model.classifier(concatenated_embeddings_torch).detach().cpu().numpy()\n",
    "\n",
    "    # Since our model outputs a LogSoftmax, find the real \n",
    "    # percentages by reversing the log function\n",
    "    whole_test_outputs[mini_batch_counter * BATCH_SIZE:(mini_batch_counter + 1) * BATCH_SIZE, :] = np.exp(test_batch_output)\n",
    "    mini_batch_counter += 1\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8nV24rlVmQ4"
   },
   "outputs": [],
   "source": [
    "# Get Prediction on Validation\n",
    "\n",
    "# Get Prediction on Test\n",
    "whole_test_predictions = torch.round(whole_test_outputs)\n",
    "\n",
    "# # Get Prediction on Test\n",
    "# PERCENTILE = 99.7\n",
    "# whole_test_predictions = np.zeros(whole_test_outputs.shape)\n",
    "# for i in range(len(whole_test_predictions)):\n",
    "#     whole_test_predictions[i, whole_test_outputs[i] > np.percentile(whole_test_outputs[i], PERCENTILE)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3uUlRtTRVmQ6"
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZWz5LtPVmQ6"
   },
   "outputs": [],
   "source": [
    "# Submission\n",
    "submission = revert_encoding(whole_test_predictions, label_dict_revert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXC-zF9DVmQ8"
   },
   "outputs": [],
   "source": [
    "np.array(submission).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1QWAsrLOVmQ_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df['Labels'] = submission\n",
    "test_df = test_df.drop(columns = 'Caption')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "etiGwlHJVmRD"
   },
   "outputs": [],
   "source": [
    "test_df.to_csv(os.path.join(dir_output, 'Submission_Model_Final_Pipeline.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RpP5Q483VmRF"
   },
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1450,
     "status": "error",
     "timestamp": 1590725676490,
     "user": {
      "displayName": "dl_ass_2 group",
      "photoUrl": "",
      "userId": "03244940590841693808"
     },
     "user_tz": -600
    },
    "id": "S-uwOcsSVmRG",
    "outputId": "2b396105-7508-47e1-c132-1289317a7d47"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "PATH = os.path.join(dir_output, 'Model_Final_Pipeline.pth')\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "xVGOpzCemAlY",
    "BQ5awb19VmQ2",
    "3uUlRtTRVmQ6"
   ],
   "name": "Model_V11_All_In_One.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a742eef6fb94bd6a3b2168c8dd0cd26": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4dbfc33eb9054f8da948d6484d789d9c",
      "max": 115730790,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3137518932594eb4b3116d2f78e512eb",
      "value": 115730790
     }
    },
    "121b90dfecb144408848936c69037117": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12ad520349564ad3bb9e680832cc1ca8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3137518932594eb4b3116d2f78e512eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4dbfc33eb9054f8da948d6484d789d9c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "638d7a4fb0d44c69ab6b1aad232197eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0a742eef6fb94bd6a3b2168c8dd0cd26",
       "IPY_MODEL_b075521abc2242319b602329d96ed8cb"
      ],
      "layout": "IPY_MODEL_121b90dfecb144408848936c69037117"
     }
    },
    "b075521abc2242319b602329d96ed8cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfbb4d6847b44a8080e0e862f5cde5c4",
      "placeholder": "​",
      "style": "IPY_MODEL_12ad520349564ad3bb9e680832cc1ca8",
      "value": " 110M/110M [00:01&lt;00:00, 105MB/s]"
     }
    },
    "cfbb4d6847b44a8080e0e862f5cde5c4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
